一、 基本环境介绍及基本环境配置
节点1： t1      192.168.113.120     centos6.5_64    添加1G新硬盘
节点2： t2     192.168.113.121     centos6.5_64    添加1G新硬盘
节点1与节点2均需配置
修改主机名：
    vim /etc/sysconfig/network  
    HOSTNAME=node1.hulala.com  
配置hosts解析：
    vim /etc/hosts  
    192.168.1.35    node1.hulala.com node1  
    192.168.1.36    node2.hulala.com node2    
同步系统时间:
    ntpdate cn.pool.ntp.org 
    
关闭防火墙与SELINUX
    service iptables stop  
    chkconfig iptables off  
    cat /etc/sysconfig/selinux  
    SELINUX=disabled  
以上配置在两个节点都需要配置，配置完成之后重启两个节点 
**********************************************************************
二:配置ssh互信
    [root@node1～]#ssh-keygen -t rsa -b 1024  
    [root@node1～]#ssh-copy-id root@192.168.113.121  
    [root@node2～]#ssh-keygen -t rsa -b 1024  
    [root@node2～]#ssh-copy-id root@192.168.113.120  
**********************************************************************
三：DRBD的安装与配置（node1和node2执行相同操作）
    [root@node1～]#wget -c http://elrepo.org/linux/elrepo/el6/x86_64/RPMS/drbd84-utils-8.4.2-1.el6.elrepo.x86_64.rpm  
    [root@node1～]#wget -c http://elrepo.org/linux/elrepo/el6/x86_64/RPMS/kmod-drbd84-8.4.2-1.el6_3.elrepo.x86_64.rpm  
    [root@node1～]#rpm -ivh *.rpm  
    -----------------------------------------------------------------------------------------------------------------
    centos6 
    #rpm -Uvh http://www.elrepo.org/elrepo-release-6-8.el6.elrepo.noarch.rpm 
    #yum -y install drbd84-utils kmod-drbd84
    centos7
    # rpm --import http://elrepo.org/RPM-GPG-KEY-elrepo.org
    # rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm
    # yum -y install drbd84-utils kmod-drbd84
    -----------------------------------------------------------------------------------------------------------------
    获取一个sha1值做为shared-secret
    [root@node1～]#sha1sum /etc/drbd.conf  
    8a6c5f3c21b84c66049456d34b4c4980468bcfb3  /etc/drbd.conf 
    -----------------------------------------------------------------------------------------------------------------
    配置/etc/drbd.d/global-common.conf
global {
        usage-count no;
        # minor-count dialog-refresh disable-ip-verification
}

common {
        protocol C;

        handlers {
                pri-on-incon-degr "/usr/lib/drbd/notify-pri-on-incon-degr.sh; /usr/lib/drbd/notify-emergency-reboot.sh; echo b > /proc/sysrq-trigger ; reboot -f";
                pri-lost-after-sb "/usr/lib/drbd/notify-pri-lost-after-sb.sh; /usr/lib/drbd/notify-emergency-reboot.sh; echo b > /proc/sysrq-trigger ; reboot -f";
                local-io-error "/usr/lib/drbd/notify-io-error.sh; /usr/lib/drbd/notify-emergency-shutdown.sh; echo o > /proc/sysrq-trigger ; halt -f";
                # fence-peer "/usr/lib/drbd/crm-fence-peer.sh";
                # split-brain "/usr/lib/drbd/notify-split-brain.sh root";
                # out-of-sync "/usr/lib/drbd/notify-out-of-sync.sh root";
                # before-resync-target "/usr/lib/drbd/snapshot-resync-target-lvm.sh -p 15 -- -c 16k";
                # after-resync-target /usr/lib/drbd/unsnapshot-resync-target-lvm.sh;
        }

        startup {
                #wfc-timeout 120;
                #degr-wfc-timeout 120;
        }

        disk {
                on-io-error detach;
                #fencing resource-only;
        }

        net {
                cram-hmac-alg "sha1";
                shared-secret "8a6c5f3c21b84c66049456d34b4c4980468bcfb3";
        }

        syncer {
                rate 1000M;
        }
}
        -----------------------------------------------------------------------------------------------------------------
	2、定义一个资源/etc/drbd.d/web.res，内容如下：
resource mydrbd {
  on node1.magedu.com {
    device    /dev/drbd0;
    disk      /dev/sda5;
    address   172.16.100.7:7789;
    meta-disk internal;
  }
  on node2.magedu.com {
    device    /dev/drbd0;
    disk      /dev/sda5;
    address   172.16.100.8:7789;
    meta-disk internal;
  }
} 
        以上文件在两个节点上必须相同，因此，可以基于ssh将刚才配置的文件全部同步至另外一个节点。
	--------------------------------------------------------------------------------------
	创建资源及文件系统:
        创建分区(未格式化过)
	在node1和node2
	[#root@node1～] fdisk /dev/sdb  
	在node1和node2上给资源(dbcluster)创建meta data：
        [root@node1～drbd]# drbdadm create-md dbcluster

        首先确保drbd module已经加载
        查看是否加载:
        # lsmod | grep drbd  
        若未加载,则需加载:
        # modprobe drbd  
        # lsmod | grep drbd  
        drbd                  317261  0  
        libcrc32c               1246  1 drbd  

	启动drbd后台进程:
	service   drbd   start    
	或者
	[root@node1 drbd]# drbdadm up dbcluster  
        [root@node2 drbd]# drbdadm up dbcluster
	查看启动状态：
        [root@node2 drbd]#cat /proc/drbd
	也可以使用drbd-overview命令来查看
	设置主节点
	从上面的信息中可以看出此时两个节点均处于Secondary状态。于是，我们接下来需要将其中一个节点设置为Primary。在要设置为Primary的节点上执行如下命令：
	# drbdadm primary --force mydrbd
	#drbdadm primary  mydrbd
        也可以在要设置为Primary的节点上使用如下命令来设置主节点：
	# drbdadm -- --overwrite-data-of-peer primary mydrbd
	而后再次查看状态，可以发现数据同步过程已经开始：
	# drbd-overview 
	  0:web  SyncSource Primary/Secondary UpToDate/Inconsistent C r---- 
        [============>.......] sync'ed: 66.2% (172140/505964)K delay_probe: 35
	--------------------------------------------------------------------------------
	创建文件系统
        文件系统的挂载只能在Primary节点进行，因此，也只有在设置了主节点后才能对drbd设备进行格式化：
	# mke2fs -j -L DRBD /dev/drbd0
	# mkdir /mnt/drbd 
	# mount /dev/drbd0 /mnt/drbd
	---------------------------------------------------------------------------------
	切换Primary和Secondary节点

	对主Primary/Secondary模型的drbd服务来讲，在某个时刻只能有一个节点为Primary，因此，要切换两个节点的角色，只能在先将原有的Primary节点设置为Secondary后，才能原来的Secondary节点设置为Primary:

	Node1:
	# cp -r /etc/drbd.* /mnt/drbd  
	# umount /mnt/drbd
	# drbdadm secondary web    --将节点设置为从节点
	查看状态：
	# drbd-overview 
	  0:web  Connected Secondary/Secondary UpToDate/UpToDate C r---- 

	Node2:
	# drbdadm primary web
	# drbd-overview 
	  0:web  Connected Primary/Secondary UpToDate/UpToDate C r---- 
	# mkdir /mnt/drbd
	# mount /dev/drbd0 /mnt/drbd

	使用下面的命令查看在此前在主节点上复制至此设备的文件是否存在：
	# ls /mnt/drbd
        drbd  配置完成
四：mysql的安装

	1.在node1和node2节点安装mysql:
	    yum install -y   mysql   mysql-devel  mysql-server 
	2.node1和node2都操作停止mysql服务
	    [root@node1～]# service mysql stop  
	    Shutting down MySQL.        [  OK  ] 
	3.关闭mysql临时挂载DRBD文件系统到主节点(Node1)
	    [root@node1 ~]# mount /dev/drbd0  /mnt/drbd/  
	4.node1和node2都操作修改my.cnf文件修改
	    在[mysqld]下添加新的数据存放路径
            datadir=/mysql/data  
	5.将默认的数据路径下的所有文件和目录cp到新的目录下（node2不用操作）
	    [root@host1 mysql]#cd /var/lib/  
	    [root@host1 mysql]#cp -rf mysql    /mnt/drbd/ 
	   node1和node2都操作这里注意copy过去的目录权限属主需要修改为mysql,这里直接修改mysql目录即可.
	    [root@host1 mysql]# chown -R mysql:mysql /mysql  
	6.启动node1上的mysql进行登陆测试
            [root@host1 mysql]# mysql  
	7.在节点Node1卸载DRBD文件系统
	    [root@node1 ~]# umount /var/lib/mysql_drbd  
	    [root@node1 ~]# drbdadm secondary mydrbd 
	8.将DRBD文件系统挂载节点Node2
	    [root@node2 ~]# drbdadm primary mydrbd 
	    [root@node2 ~]# mount /dev/drbd0  /mnt/drbd/
五：Corosync和Pacemaker的安装配置（node1和node2都需安装）
        1.安装 
	    yum  -y install  cluster-glue*  corosync  corosynclib   heartbeat   heartbeatlib  libesmtp pacemaker   packmaker-lib  pacemaker-cts resource-agents
        2、配置corosync，（以下命令在node1.magedu.com上执行）
           # cd /etc/corosync
           # cp corosync.conf.example corosync.conf
	   [root@node1 coorosync]# grep -vE "^([[:space:]]*#)|^$" /etc/corosync/corosync.conf
compatibility: whitetank    #表示是否支持0.8之前的版本
totem {    #图腾，这是用来定义集群中各节点中是怎么通信的
        version: 2    #各节点互相通信的协议的版本
        secauth: on    #各节点间通信是否需要认证
        threads: 0    #使用多少线程进行加密和发送多播信息
        interface {
                ringnumber: 0    #环号码，避免消息环路产生
                bindnetaddr: 192.168.220.0    #绑定的网络地址
                mcastaddr: 239.255.1.1    #多播地址
                mcastport: 5405    #多播端口
                ttl: 1    
        }
}
logging {    #日志相关
        fileline: off
        to_stderr: no
        to_logfile: yes
        logfile: /var/log/cluster/corosync.log
        to_syslog: no
        debug: off
        timestamp: on
        logger_subsys {
                subsys: AMF
                debug: off
        }
}
service {    #定义启动corosync时启动pacemaker
        ver: 0
        name: pacemaker
}
aisexec {    #定义以哪个用户和组启动corosync
        user: root
        group: root
}

接着编辑corosync.conf，添加如下内容：
service {
  ver:  0
  name: pacemaker
  
}

aisexec {
  user: root
  group:  root
}
--------------------------------------------------------------------------------------------
         3.生成节点间通信时用到的认证密钥文件：
           # corosync-keygen
           将corosync和authkey复制至node2:
           # scp -p corosync authkey  node2:/etc/corosync/
	 4.安装crm
	 5.启动corosync（以下命令在node1上执行）：
           # /etc/init.d/corosync start
	 6.查看corosync引擎是否正常启动：
		# grep -e "Corosync Cluster Engine" -e "configuration file" /var/log/cluster/corosync.log 
		Sep 15 11:37:39 corosync [MAIN  ] Corosync Cluster Engine ('1.4.1'): started and ready to provide service.
		Sep 15 11:37:39 corosync [MAIN  ] Successfully read main configuration file '/etc/corosync/corosync.conf'.

		查看初始化成员节点通知是否正常发出：
		# grep  TOTEM  /var/log/cluster/corosync.log
		Sep 15 11:37:39 corosync [TOTEM ] Initializing transport (UDP/IP Multicast).
		Sep 15 11:37:39 corosync [TOTEM ] Initializing transmit/receive security: libtomcrypt SOBER128/SHA1HMAC (mode 0).
		Sep 15 11:37:39 corosync [TOTEM ] The network interface [172.16.100.15] is now up.
		Sep 15 11:37:39 corosync [TOTEM ] A processor joined or left the membership and a new membership was formed.
		Sep 15 11:37:39 corosync [TOTEM ] A processor joined or left the membership and a new membership was formed.

		检查启动过程中是否有错误产生。下面的错误信息表示packmaker不久之后将不再作为corosync的插件运行，因此，建议使用cman作为集群基础架构服务；此处可安全忽略。
		# grep ERROR: /var/log/cluster/corosync.log | grep -v unpack_resources
		Sep 15 11:37:39 corosync [pcmk  ] ERROR: process_ais_conf: You have configured a cluster using the Pacemaker plugin for Corosync. The plugin is not supported in this environment and will be removed very soon.
		Sep 15 11:37:39 corosync [pcmk  ] ERROR: process_ais_conf:  Please see Chapter 8 of 'Clusters from Scratch' (http://www.clusterlabs.org/doc) for details on using Pacemaker with CMAN
		Sep 15 11:37:40 corosync [pcmk  ] ERROR: pcmk_wait_dispatch: Child process mgmtd exited (pid=2375, rc=100)

		查看pacemaker是否正常启动：
		# grep pcmk_startup /var/log/cluster/corosync.log 
		Sep 15 11:37:39 corosync [pcmk  ] info: pcmk_startup: CRM: Initialized
		Sep 15 11:37:39 corosync [pcmk  ] Logging: Initialized pcmk_startup
		Sep 15 11:37:39 corosync [pcmk  ] info: pcmk_startup: Maximum core file size is: 18446744073709551615
		Sep 15 11:37:39 corosync [pcmk  ] info: pcmk_startup: Service: 9
		Sep 15 11:37:39 corosync [pcmk  ] info: pcmk_startup: Local hostname: node1.magedu.com

		如果上面命令执行均没有问题，接着可以执行如下命令启动node2上的corosync
		# ssh node2 -- /etc/init.d/corosync start	
		如果安装了crmsh，可使用如下命令查看集群节点的启动状态：
		# crm status
六、资源配置
          1.配置资源及约束                 
            配置默认属性
	    禁止STONITH错误:
	    [root@node1 ~]# crm configure property stonith-enabled=false  
	    [root@node1 ~]# crm_verify -L  
           让集群忽略Quorum:
            [root@node1~]# crm configure property no-quorum-policy=ignore
	    防止资源在恢复之后移动:
            [root@node1~]# crm configure rsc_defaults resource-stickiness=100 
	    设置操作的默认超时:
            设置默认的启动失败是否为致命的:
            [root@node1~]# crm configure property start-failure-is-fatal="false" 
	    [root@node1~]# crm configure property default-action-timeout="180s"
	   2.配置drbd
	     配置DRBD资源:
	     primitive mydrbd ocf:linbit:drbd params drbd_resource=mydrbd op  start  timeout=240 op stop  timeout=100  op monitor role=Master interval=10s timeout=30s op monitor role=Slave interval=20s timeout=30s
	     配置DRBD资源主从关系(定义只有一个Master节点):
	     master MS_drbd mydrbd meta master-max="1" master-node-max="1" clone-max="2" clone-node-max="1" notify="true"
             配置文件系统资源,定义挂载点(mount point):
	     crm(live)configure# primitive fs_drbd ocf:heartbeat:Filesystem params device="/dev/drbd0" directory="/mnt/drbd" fstype="ext3" op  start  timeout=100 op stop  timeout=100
	    3.配置VIP资源
	     crm(live)configure# primitive vip ocf:heartbeat:IPaddr2 params ip="192.168.113.110" cidr_netmask="24"
	    4.配置MySQL资源
	     crm(live)configure# primitive mysqld lsb:mysqld op monitor interval="20s" timeout="30s" op start interval="0" timeout="180s" op stop interval="0" timeout="240s" 
七、组资源和约束
            通过”组”确保DRBD,MySQL和VIP是在同一个节点(Master)并且确定资源的启动/停止顺序.
	        启动: p_fs_mysqlC>p_ip_mysql->p_mysql  
		停止: p_mysqlC>p_ip_mysqlC>p_fs_mysql 
	    crm(live)configure# group group_mysql fs_drbd vip mysqld 
	    组group_mysql永远只在Master节点:
	    crm(live)configure# colocation c_mysql_on_drbd inf: group_mysql MS_drbd:Master
	    MySQL的启动永远是在DRBD Master之后:
	    crm(live)configure# order drbd_before_mysql inf: MS_drbd:promote group_mysql:start
	    配置检查和提交
	    crm(live)configure# verify  
	    crm(live)configure# commit  
	    crm(live)configure# quit  